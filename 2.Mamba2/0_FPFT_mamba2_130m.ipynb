{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import transformers\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from datasets import load_dataset, Dataset, DatasetDict, load_from_disk\n",
    "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import Trainer, AutoTokenizer, Mamba2ForCausalLM, TrainingArguments, DataCollatorWithPadding, Mamba2Config, GenerationMixin, DataCollatorForMultipleChoice, DataCollatorForLanguageModeling\n",
    "from transformers.models.mamba2.modeling_mamba2 import Mamba2PreTrainedModel, Mamba2Model, Mamba2Cache, Mamba2CausalLMOutput\n",
    "from transformers.utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings, add_code_sample_docstrings\n",
    "import evaluate\n",
    "\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "import peft.tuners.lora.layer as pl\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "\n",
    "import pennylane as qml\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "_CHECKPOINT_FOR_DOC = \"mistralai/mamba-codestral-7B-v0.1\"\n",
    "_CONFIG_FOR_DOC = \"Mamba2Config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name=\"state-spaces/mamba-130m-hf\"\n",
    "model_name=\"AntonV/mamba2-130m-hf\"\n",
    "output_dir = \"mamba2_cp/0_iot_fpft_mamba2_130m\"\n",
    "\n",
    "lora_r = 2\n",
    "train_batch_size = 1\n",
    "eval_batch_size = 1\n",
    "learning_rate = 5e-4\n",
    "num_train_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" #enforce padding side left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Timestamp\": \"2024-03-25 12:00:00\", \"Device_ID\": \"D14\", \"Temp\": \"32°C\", \"Humidity\": \"72%\", \"Workload_Type\": \"Data Analytics\", \"Processing_Tier\": \"Device\", \"CPU_Usage(%)\": 24, \"Memory_Usage(MB)\": 2276, \"Network_Latency(ms)\": 29, \"Network_Jitter(ms)\": 8, \"Task_Execution_Time(ms)\": 189, \"Predicted_Resource_Allocation(%)\": 54}\n",
      "###\n",
      "Predict the Actual Resource Allocation(%)\n",
      "\n",
      "Actual Resource Allocation(%):\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "df_dataset = pd.read_csv(\"iot_datasets/iot_resource_allocation_dataset.csv\")\n",
    "def row_to_json_prompt(r):\n",
    "    payload = {\n",
    "        \"Timestamp\":               r[\"Timestamp\"],\n",
    "        \"Device_ID\":               r[\"Device_ID\"],\n",
    "        \"Temp\":                    r[\"Sensor_Data\"].split(\",\")[0].split(\":\")[1].strip(),\n",
    "        \"Humidity\":                r[\"Sensor_Data\"].split(\",\")[1].split(\":\")[1].strip(),\n",
    "        \"Workload_Type\":           r[\"Workload_Type\"],\n",
    "        \"Processing_Tier\":         r[\"Processing_Tier\"],\n",
    "        \"CPU_Usage(%)\":            r[\"CPU_Usage(%)\"],\n",
    "        \"Memory_Usage(MB)\":        r[\"Memory_Usage(MB)\"],\n",
    "        \"Network_Latency(ms)\":     r[\"Network_Latency(ms)\"],\n",
    "        \"Network_Jitter(ms)\":      r[\"Jitter(ms)\"],\n",
    "        \"Task_Execution_Time(ms)\": r[\"Task_Execution_Time(ms)\"],\n",
    "        \"Predicted_Resource_Allocation(%)\": r[\"Predicted_Resource_Allocation(%)\"],\n",
    "    }\n",
    "    return f\"{json.dumps(payload, ensure_ascii=False)}\\n###\\nPredict the Actual Resource Allocation(%)\"\n",
    "\n",
    "df_dataset[\"text\"]  = df_dataset.apply(row_to_json_prompt, axis=1)\n",
    "df_dataset[\"label\"] = df_dataset[\"Actual_Resource_Allocation(%)\"].astype(str)          \n",
    "df = df_dataset[[\"text\", \"label\"]]    \n",
    "\n",
    "print(df[\"text\"][0])\n",
    "print()\n",
    "print(\"Actual Resource Allocation(%):\")\n",
    "print(df[\"label\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➜  载入已缓存的数据集 …\n"
     ]
    }
   ],
   "source": [
    "CACHE_PATH = \"iot_datasets/iot_resource_allocation_hf_dataset\"\n",
    "\n",
    "if os.path.exists(CACHE_PATH):\n",
    "    print(\"➜  Load the cached dataset …\")\n",
    "    ds_dict = load_from_disk(CACHE_PATH)          # DatasetDict(train, test)\n",
    "else:\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "    ds_dict = DatasetDict({\n",
    "        \"train\": Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
    "        \"test\":  Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "    })\n",
    "\n",
    "    ds_dict.save_to_disk(CACHE_PATH)\n",
    "    print(f\"✓ Save to {CACHE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '{\"Timestamp\": \"2024-03-25 12:00:00\", \"Device_ID\": \"D14\", \"Temp\": \"32°C\", \"Humidity\": \"72%\", \"Workload_Type\": \"Data Analytics\", \"Processing_Tier\": \"Device\", \"CPU_Usage(%)\": 24, \"Memory_Usage(MB)\": 2276, \"Network_Latency(ms)\": 29, \"Network_Jitter(ms)\": 8, \"Task_Execution_Time(ms)\": 189, \"Predicted_Resource_Allocation(%)\": 54}\\n###\\nPredict the Actual Resource Allocation(%)', 'label': '51'}\n"
     ]
    }
   ],
   "source": [
    "iot_train_hf_dataset = Dataset.from_list(ds_dict[\"train\"])\n",
    "print(iot_train_hf_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw longest length: 141\n",
      "Effective MAX_LEN: 141\n"
     ]
    }
   ],
   "source": [
    "def length_only(example):\n",
    "    ids = tokenizer(example[\"text\"], add_special_tokens=True,\n",
    "                    padding=False, truncation=False).input_ids\n",
    "    return {\"seq_len\": len(ids)}\n",
    "\n",
    "train_with_len = ds_dict[\"train\"].map(length_only)\n",
    "MAX_LEN = int(np.max(train_with_len[\"seq_len\"]))\n",
    "print(\"Raw longest length:\", MAX_LEN)\n",
    "\n",
    "MAX_LEN = min(MAX_LEN, tokenizer.model_max_length)            \n",
    "print(\"Effective MAX_LEN:\", MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tokenize input_ids / attention_mask / labels\n",
    "def tok_fn(batch):\n",
    "    prompt_ids = tokenizer(batch[\"text\"], truncation=True, max_length=MAX_LEN).input_ids\n",
    "    answer_ids = tokenizer(batch[\"label\"], add_special_tokens=False).input_ids\n",
    "    input_ids  = [p + answer_ids[i] + [tokenizer.eos_token_id] for i, p in enumerate(prompt_ids)]\n",
    "    labels     = [[-100]*len(p) + answer_ids[i] + [tokenizer.eos_token_id] for i, p in enumerate(prompt_ids)]\n",
    "    attn_mask  = [[1]*len(ids) for ids in input_ids]\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attn_mask, \"labels\": labels}\n",
    "\n",
    "# 2. DatasetDict\n",
    "tokd = ds_dict.map(\n",
    "    tok_fn,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\", \"label\"]   \n",
    ")\n",
    "\n",
    "tokd.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(features):\n",
    "    input_ids_batch = []\n",
    "    attention_mask_batch = []\n",
    "    labels_batch = []\n",
    "\n",
    "    for f in features: \n",
    "        ids = f[\"input_ids\"]\n",
    "        if isinstance(ids, torch.Tensor):\n",
    "            ids = ids.detach().clone()\n",
    "        else:\n",
    "            ids = torch.tensor(ids, dtype=torch.long)\n",
    "        input_ids_batch.append(ids)\n",
    "\n",
    "        mask = f[\"attention_mask\"]\n",
    "        if isinstance(mask, torch.Tensor):\n",
    "            mask = mask.detach().clone()\n",
    "        else:\n",
    "            mask = torch.tensor(mask, dtype=torch.long)\n",
    "        attention_mask_batch.append(mask)\n",
    "\n",
    "        lab = f[\"labels\"]\n",
    "        if isinstance(lab, torch.Tensor):\n",
    "            lab = lab.detach().clone()\n",
    "        else:\n",
    "            lab = torch.tensor(lab, dtype=torch.long)\n",
    "        labels_batch.append(lab)\n",
    "\n",
    "    input_ids = pad_sequence(\n",
    "        input_ids_batch, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    attention_mask = pad_sequence(\n",
    "        attention_mask_batch, batch_first=True, padding_value=0\n",
    "    )\n",
    "    labels = pad_sequence(\n",
    "        labels_batch, batch_first=True, padding_value=-100\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mamba2ForCausalLM(\n",
      "  (backbone): Mamba2Model(\n",
      "    (embeddings): Embedding(50288, 768)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Mamba2Block(\n",
      "        (norm): Mamba2RMSNorm()\n",
      "        (mixer): Mamba2Mixer(\n",
      "          (act): SiLU()\n",
      "          (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
      "          (in_proj): Linear(in_features=768, out_features=3352, bias=False)\n",
      "          (norm): MambaRMSNormGated()\n",
      "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm_f): Mamba2RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50288, bias=False)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50277, 768)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Mamba2ForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"cuda\",\n",
    "    )\n",
    "print(model)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = evaluate.load(\"mse\")                                     \n",
    "mae  = evaluate.load(\"mae\")\n",
    "\n",
    "def post_process(texts):\n",
    "    outs = []\n",
    "    for t in texts:\n",
    "        try:\n",
    "            outs.append(float(t.strip().split()[-1]))\n",
    "        except ValueError:\n",
    "            outs.append(np.nan)\n",
    "    return np.array(outs)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    decoded = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    y_pred  = post_process(decoded)\n",
    "    y_true  = post_process(tokenizer.batch_decode(labels, skip_special_tokens=True))\n",
    "\n",
    "    mask = ~np.isnan(y_pred)\n",
    "    y_pred, y_true = y_pred[mask], y_true[mask]\n",
    "\n",
    "    rmse_v = rmse.compute(predictions=y_pred, references=y_true, squared=False)[\"mse\"]\n",
    "    mae_v  = mae.compute(predictions=y_pred, references=y_true)[\"mae\"]\n",
    "    eff    = (np.abs(y_pred - y_true) <= 2).mean()\n",
    "    return {\"rmse\": rmse_v, \"mae\": mae_v, \"efficiency\": eff}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments and trials welcome!\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    save_steps=5000,\n",
    "    logging_steps=100,\n",
    "    # logging_first_step=True,\n",
    "    fp16=False,          \n",
    "    bf16=False,           \n",
    "    save_safetensors=False,\n",
    "    resume_from_checkpoint=True,\n",
    "    dataloader_drop_last = True,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=num_train_epochs,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=tokd[\"train\"],\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 128981184\n",
      "Trainable params: 128981184\n",
      "Non-trainable params: 0\n",
      "Mamba2ForCausalLM(\n",
      "  (backbone): Mamba2Model(\n",
      "    (embeddings): Embedding(50277, 768)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Mamba2Block(\n",
      "        (norm): Mamba2RMSNorm()\n",
      "        (mixer): Mamba2Mixer(\n",
      "          (act): SiLU()\n",
      "          (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
      "          (in_proj): Linear(in_features=768, out_features=3352, bias=False)\n",
      "          (norm): MambaRMSNormGated()\n",
      "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm_f): Mamba2RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50277, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = trainer.model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "non_trainable_params = total_params - trainable_params\n",
    "\n",
    "print(f\"Total params: {total_params}\")\n",
    "print(f\"Trainable params: {trainable_params}\")\n",
    "print(f\"Non-trainable params: {non_trainable_params}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8000' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8000/8000 12:05, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.256700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.685800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.708100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.595000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.577400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.562300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.535400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.463400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.474900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.553200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.482300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.513300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.427100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.443800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.408500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.381600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.360600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.852800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.766600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.832600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.546600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.405300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.347300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.404200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.385800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.315200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.357800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.384700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.363700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.360500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.281000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.258900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.314100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.338100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.300600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.313400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.387400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.307700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.230500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.241500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.283800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.212100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.270400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.297500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.263200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.238200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.180600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.224700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.229900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>1.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.226700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.234500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.177700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>1.066000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.105600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>1.150500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.136000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>1.128200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.169800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>1.155200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.147100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.027500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.990300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.930400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.916600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>1.106500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.009800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.926700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.012800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.644600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.878000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.830600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.690400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.644800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.948500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.761200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.763300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8000, training_loss=1.303566107749939, metrics={'train_runtime': 741.2941, 'train_samples_per_second': 10.792, 'train_steps_per_second': 10.792, 'total_flos': 616883943951360.0, 'train_loss': 1.303566107749939, 'epoch': 10.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "# trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mamba2_cp/0_iot_fpft_mamba2_130m/tokenizer_config.json',\n",
       " 'mamba2_cp/0_iot_fpft_mamba2_130m/special_tokens_map.json',\n",
       " 'mamba2_cp/0_iot_fpft_mamba2_130m/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mamba(model, context):\n",
    "\n",
    "    text = f\"{context}\"\n",
    "    # print(text)\n",
    "    # input_ids = torch.LongTensor([tokenizer.encode(text)]).cuda()\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\").cuda()\n",
    "    # print(input_ids)\n",
    "\n",
    "    attention_mask = torch.ones_like(input_ids).cuda()\n",
    "\n",
    "    # print(\"max_length\", input_ids.shape[1])\n",
    "\n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=input_ids.shape[1]+10, #max_length_in_dataset\n",
    "        # max_length=max_length_in_dataset,\n",
    "        # max_new_tokens=max_length_in_dataset,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # print(out)\n",
    "    decoded = tokenizer.batch_decode(out)[0]\n",
    "    # print(\"=\"*80)\n",
    "    # print(decoded)\n",
    "\n",
    "    # out returns the whole sequence plus the original\n",
    "    cleaned = decoded.replace(text, \"\")\n",
    "    # cleaned = decoded[len(text):]\n",
    "    cleaned = cleaned.replace(\"<|endoftext|>\", \"\")\n",
    "\n",
    "    # the model will just keep generating, so only grab the first one\n",
    "    cleaned = cleaned.split(\"\\n\\n\")[0].strip()\n",
    "    lines = cleaned.splitlines()\n",
    "    if lines:\n",
    "        cleaned = lines[0].strip()\n",
    "\n",
    "    # print(answer)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "context=\"{\\\"Timestamp\\\": \\\"2024-03-25 12:00:00\\\", \\\"Device_ID\\\": \\\"D14\\\", \\\"Temp\\\": \\\"32°C\\\", \\\"Humidity\\\": \\\"72%\\\", \\\"Workload_Type\\\": \\\"Data Analytics\\\", \\\"Processing_Tier\\\": \\\"Device\\\", \\\"CPU_Usage(%)\\\": 24, \\\"Memory_Usage(MB)\\\": 2276, \\\"Network_Latency(ms)\\\": 29, \\\"Network_Jitter(ms)\\\": 8, \\\"Task_Execution_Time(ms)\\\": 189, \\Predicted_Resource_Allocation(%)\\\": 54}\\n###\\nPredict the Actual Resource Allocation(%)\"\n",
    "print(run_mamba(model, context=context))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
